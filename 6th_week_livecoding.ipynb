{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60457791",
   "metadata": {},
   "source": [
    "# Autotagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e73fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97164c88",
   "metadata": {},
   "source": [
    "- Download Small MTAT if you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea84fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown==4.4.0\n",
    "!gdown 15e9E3oZdudErkPKwb0rCAiZXkPxdZkV6\n",
    "!unzip -q mtat_8000.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de7e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTATDataset:\n",
    "  def __init__(self, dir_path, split='train', num_max_data=6000, sr=16000):\n",
    "    self.dir = Path(dir_path)\n",
    "    self.labels = pd.read_csv(self.dir / \"meta.csv\", index_col=[0])\n",
    "    self.sr = sr\n",
    "\n",
    "    if split==\"train\":\n",
    "      sub_dir_ids = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c']\n",
    "    elif split=='valid':\n",
    "      sub_dir_ids = ['d']\n",
    "    else: #test\n",
    "      sub_dir_ids = ['e', 'f', 'g']\n",
    "\n",
    "    is_in_set = [True if x[0] in sub_dir_ids else False for x in self.labels['mp3_path'].values.astype('str')]\n",
    "    self.labels = self.labels.iloc[is_in_set]\n",
    "    self.labels = self.labels[:num_max_data]\n",
    "    self.vocab = self.labels.columns.values[1:-1]\n",
    "    self.label_tensor = self.convert_label_to_tensor()\n",
    "  \n",
    "  def convert_label_to_tensor(self):\n",
    "    return torch.LongTensor(self.labels.values[:, 1:-1].astype('bool'))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "data_dir = Path('MTAT_SMALL')\n",
    "dataset= MTATDataset(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4074a82",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc3b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnFlyDataset(MTATDataset):\n",
    "  def __init__(self, dir_path, split='train', num_max_data=6000, sr=16000):\n",
    "    super().__init__(dir_path, split, num_max_data, sr)\n",
    "#     self.resampler = torchaudio.transforms.Resample(orig_freq=22050, new_freq=self.sr)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    audio_path = self.labels['mp3_path'].iloc[idx]\n",
    "    y, sr = torchaudio.load(self.dir / audio_path)\n",
    "    \n",
    "    if sr != self.sr:\n",
    "      y = torchaudio.functional.resample(y, orig_freq=sr, new_freq=self.sr)\n",
    "    \n",
    "    label = self.label_tensor[idx]\n",
    "    return y[0], label\n",
    "    \n",
    "\n",
    "trainset = OnFlyDataset(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f48eb7",
   "metadata": {},
   "source": [
    "## Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35d280b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SpecModel(nn.Module):\n",
    "  def __init__(self, sr, n_fft, hop_length, n_mels):\n",
    "    super().__init__()\n",
    "    self.mel_converter = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    self.db_converter = torchaudio.transforms.AmplitudeToDB()\n",
    "  def forward(self, x):\n",
    "    mel_spec = self.mel_converter(x)\n",
    "    return self.db_converter(mel_spec)\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "  def __init__(self, sr, n_fft, hop_length, n_mels, hidden_size, num_output):\n",
    "    super().__init__()\n",
    "    self.sr = sr\n",
    "    self.spec_converter = SpecModel(sr, n_fft, hop_length, n_mels)\n",
    "    self.conv_layer = nn.Sequential(\n",
    "      nn.Conv1d(n_mels, out_channels=hidden_size, kernel_size=3),\n",
    "      nn.MaxPool1d(3),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
    "      nn.MaxPool1d(3),\n",
    "      nn.ReLU(),     \n",
    "      nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
    "      nn.MaxPool1d(3),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    self.final_layer = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "  def get_spec(self, x):\n",
    "    '''\n",
    "    Get result of self.spec_converter\n",
    "    x (torch.Tensor): audio samples (num_batch_size X num_audio_samples)\n",
    "    '''\n",
    "    return self.spec_converter(x)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    spec = self.get_spec(x) # num_batch X num_mel_bins X num_time_bins\n",
    "    out = self.conv_layer(spec)\n",
    "    out = torch.max(out, dim=-1)[0] # select [0] because torch.max outputs tuple of (value, index)\n",
    "    out = self.final_layer(out)\n",
    "    out = torch.sigmoid(out)\n",
    "    return out\n",
    "  \n",
    "model = AudioModel(16000, 1024, 512, 40, 128, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7703d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 40\n",
    "hidden_size = 64\n",
    "\n",
    "seq_conv_layer = nn.Sequential(\n",
    "      nn.Conv1d(n_mels, out_channels=hidden_size, kernel_size=5),\n",
    "      nn.MaxPool1d(4),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
    "      nn.MaxPool1d(3),\n",
    "      nn.ReLU(),     \n",
    "      nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
    "      nn.MaxPool1d(3),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10bef88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, label = trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f2fb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = model.spec_converter(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57624e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 911])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spec.shape # freq_bin (channel) x time_bin  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ab0a876d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 24])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out = seq_conv_layer(mel_spec) \n",
    "# model_out, \n",
    "model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "58053f71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 24])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fc7aeaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.9713, -1.7584,  3.0312,  ..., -2.3761, -3.1079,  1.4732],\n",
       "         [-1.4117, -0.7950,  0.9067,  ..., -1.1927, -1.9066,  0.0404],\n",
       "         [-1.6910, -0.9139,  1.3879,  ..., -1.1623, -1.5955,  0.2827],\n",
       "         ...,\n",
       "         [-0.9937, -0.2435,  1.1672,  ..., -1.3833, -1.3047,  0.1060],\n",
       "         [-1.0243, -0.3593,  1.2652,  ..., -1.3761, -1.3017,  0.0996],\n",
       "         [-1.4774, -0.4496,  1.2401,  ..., -1.6750, -1.4967,  0.1884]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " torch.Size([24, 50]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test = nn.Linear(64, 50, bias=False)\n",
    "test_out = final_test(model_out.T)\n",
    "test_out, test_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6f33ba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 64])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6c443dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.9713, -1.7584,  3.0312,  ..., -2.3761, -3.1079,  1.4732],\n",
       "         [-1.4117, -0.7950,  0.9067,  ..., -1.1927, -1.9066,  0.0404],\n",
       "         [-1.6910, -0.9139,  1.3879,  ..., -1.1623, -1.5955,  0.2827],\n",
       "         ...,\n",
       "         [-0.9937, -0.2435,  1.1672,  ..., -1.3833, -1.3047,  0.1060],\n",
       "         [-1.0243, -0.3593,  1.2652,  ..., -1.3761, -1.3017,  0.0996],\n",
       "         [-1.4774, -0.4496,  1.2401,  ..., -1.6750, -1.4967,  0.1884]],\n",
       "        grad_fn=<PermuteBackward0>),\n",
       " torch.Size([24, 50]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_conv = nn.Conv1d(64, 50, kernel_size=1, bias=False)\n",
    "final_conv.weight.data[:,:,0] = final_test.weight.data\n",
    "test_out = final_conv(model_out)\n",
    "test_out.T, test_out.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "73be1540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 64, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9a35622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0.0000,     2.6823,     0.0000,     1.2487,     0.8096,     0.1167,\n",
       "             0.7276,     0.0365,     0.0000,     0.0000,     0.1344,     1.8576,\n",
       "             0.0076,     1.2481,     2.8068,     0.0295,     0.0000,     0.1667,\n",
       "             0.0296,     2.3879,     0.0510,     0.0772,     3.2861,     1.5576,\n",
       "             0.4737,     3.7279,     4.4033,     4.3582,     2.3644,     2.3192,\n",
       "             0.2317,     0.5021,     0.1446,     0.0000,     0.0000,     0.6143,\n",
       "             7.1379,     0.6086,     0.0000,     0.3253,     0.0000,     2.1696,\n",
       "             0.0000,     1.1167,     5.6040,     0.0143,     0.3262,     0.0161,\n",
       "             6.1156,     0.0008,     0.0000,     0.0000,     0.4237,     1.8557,\n",
       "             3.6960,     0.8464,     0.5737,     0.0186,     0.0000,     0.0136,\n",
       "             0.0000,     0.0720,     3.8648,     0.0340],\n",
       "        grad_fn=<MeanBackward1>),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooling = torch.mean(model_out, dim=-1)\n",
    "mean_pooling, mean_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed39ca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0.0000,     4.3618,     0.0000,     1.7843,     1.6946,     1.2563,\n",
       "             1.9779,     0.8471,     0.0000,     0.0000,     1.3759,     4.5051,\n",
       "             0.1820,     2.2768,     3.9449,     0.3172,     0.0000,     3.4557,\n",
       "             0.2560,     9.2989,     0.5654,     1.0495,     8.2116,     2.4553,\n",
       "             2.4427,    10.3835,     7.7901,    10.0643,     2.8923,     6.2799,\n",
       "             0.9667,     1.9399,     1.1106,     0.0000,     0.0000,     6.5254,\n",
       "            12.6467,     1.4461,     0.0000,     0.9304,     0.0000,     7.5213,\n",
       "             0.0000,     2.1392,     8.5439,     0.3008,     1.1191,     0.2249,\n",
       "            10.7428,     0.0197,     0.0000,     0.0000,     1.3934,     8.3979,\n",
       "             9.0301,     4.3973,     2.4469,     0.4472,     0.0000,     0.2360,\n",
       "             0.0000,     0.9428,     8.2263,     0.8164],\n",
       "        grad_fn=<MaxBackward0>),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pooling = torch.max(model_out, dim=-1).values\n",
    "max_pooling, max_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ac87795",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3051, -2.8393, -0.2600, -1.5165, -4.5268, -2.7030,  0.5587,  2.5728,\n",
       "        -0.6841, -2.0289,  1.3267, -0.3184, -2.6279, -4.4766,  0.1575,  2.0931,\n",
       "        -0.5669,  0.9044, -2.9504, -5.9490, -0.4106,  0.5929,  0.2330,  1.7918,\n",
       "         3.3626,  2.7782, -2.5826,  2.0242, -1.9924,  3.2855,  3.5405, -2.7477,\n",
       "         1.4999,  0.0748,  3.2991,  1.1368, -0.7087, -3.0218,  6.4047, -0.3177,\n",
       "         1.0323,  3.1941,  3.6477, -4.1344,  0.0329,  4.6421,  0.3591,  2.6181,\n",
       "        -0.8550,  1.0683], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tags =50\n",
    "final_layer = nn.Linear(hidden_size, num_tags)\n",
    "\n",
    "final_layer(max_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb25fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b7ce3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cfd2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbbb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a627faf",
   "metadata": {},
   "source": [
    "## How CNN works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e567c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 3., -3., -1., -3.,  3., -1.,  0.],\n",
       "          [ 0., -2.,  3., -3., -3.,  1., -1.],\n",
       "          [ 0., -2., -3., -2.,  1., -2.,  1.],\n",
       "          [-1.,  1.,  1., -1.,  0.,  1.,  1.],\n",
       "          [ 1.,  2., -2., -1.,  1., -1., -3.],\n",
       "          [-3.,  2., -3.,  1.,  3., -1.,  2.]]]),\n",
       " torch.Size([1, 6, 7]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = torch.randint(-3, 4, (6,7)).float()\n",
    "dummy = dummy.unsqueeze(0)\n",
    "dummy, dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f444446",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv2d(1, 1, kernel_size=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "152f7835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-1.,  1.,  0.],\n",
       "          [-1.,  1.,  0.],\n",
       "          [ 1.,  1.,  1.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer.weight.shape\n",
    "conv_layer.weight.data = torch.randint(-1, 2, (1,1,3,3)).float()\n",
    "conv_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd84510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 3., -3., -1., -3.,  3., -1.,  0.],\n",
       "          [ 0., -2.,  3., -3., -3.,  1., -1.],\n",
       "          [ 0., -2., -3., -2.,  1., -2.,  1.],\n",
       "          [-1.,  1.,  1., -1.,  0.,  1.,  1.],\n",
       "          [ 1.,  2., -2., -1.,  1., -1., -3.],\n",
       "          [-3.,  2., -3.,  1.,  3., -1.,  2.]]]),\n",
       " Parameter containing:\n",
       " tensor([[[[-1.,  1.,  0.],\n",
       "           [-1.,  1.,  0.],\n",
       "           [ 1.,  1.,  1.]]]], requires_grad=True),\n",
       " tensor([[[-13.,   0., -12.,   3.,   0.],\n",
       "          [ -3.,   5.,  -5.,   3.,   3.],\n",
       "          [  1.,  -2.,  -3.,   3.,  -5.],\n",
       "          [ -1.,  -4.,   0.,   6.,   3.]]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2D Conv Layer의 입력은 3차원\n",
    "\n",
    "'''\n",
    "dummy, conv_layer.weight, conv_layer(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81c8d74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer(dummy[:, 0:3, 1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930df4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8173c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -3.,  0., -3.,  0.,  2.,  2., -1.,  3.,  1.],\n",
       "        [-2., -1.,  3., -3.,  2.,  0.,  0.,  2.,  3., -3.],\n",
       "        [ 1., -2., -3.,  0.,  1.,  3., -3.,  1.,  3., -2.],\n",
       "        [-1., -2., -2., -3.,  0., -3., -3., -1.,  1.,  3.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ch = 4\n",
    "dummy = torch.randint(-3, 4, (num_ch, 10)).float()\n",
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "205def3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-1.,  1., -1.],\n",
       "         [ 0.,  0., -1.],\n",
       "         [ 1.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[-1.,  1., -1.],\n",
       "         [-1.,  1., -1.],\n",
       "         [ 0.,  1.,  0.],\n",
       "         [ 1., -1.,  1.]],\n",
       "\n",
       "        [[ 1., -1., -1.],\n",
       "         [ 1.,  1.,  1.],\n",
       "         [-1.,  1., -1.],\n",
       "         [ 0.,  1.,  1.]],\n",
       "\n",
       "        [[-1.,  1.,  1.],\n",
       "         [-1., -1., -1.],\n",
       "         [-1., -1., -1.],\n",
       "         [ 1.,  1.,  1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.],\n",
       "         [-1.,  0.,  0.],\n",
       "         [ 0.,  1.,  1.],\n",
       "         [ 1., -1.,  1.]],\n",
       "\n",
       "        [[ 0.,  1., -1.],\n",
       "         [ 0.,  0., -1.],\n",
       "         [ 0.,  0., -1.],\n",
       "         [-1., -1.,  1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ch = 6\n",
    "conv1d = nn.Conv1d(num_ch, out_ch, kernel_size=3, bias=False)\n",
    "conv1d.weight.data = torch.randint(-1, 2, conv1d.weight.shape).float()\n",
    "conv1d.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ce029c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-10.,   7.,  -7.,   4.,  -2.,   3.,  -9.,   5.],\n",
       "         [-10.,   7., -10.,   1.,   1.,  -5.,  -7.,  11.],\n",
       "         [  1.,  -7.,   4., -11.,  -3.,  -8.,   6.,   5.],\n",
       "         [ -6.,  -1.,  -8.,  -4.,  -5., -11.,  -9.,   4.],\n",
       "         [ -5., -11.,  -4.,   0.,   2.,   0.,   7.,   3.],\n",
       "         [ -2.,   7.,  -1.,  -5.,   3.,   5.,  -5.,  10.]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " torch.Size([6, 8]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv1d(dummy)\n",
    "out, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fb1c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool_layer = nn.MaxPool1d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c416eee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.,  4.],\n",
       "         [ 7.,  1.],\n",
       "         [ 4., -3.],\n",
       "         [-1., -4.],\n",
       "         [-4.,  2.],\n",
       "         [ 7.,  5.]], grad_fn=<SqueezeBackward1>),\n",
       " torch.Size([6, 2]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_pool = max_pool_layer(out)\n",
    "after_pool, after_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73672ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
